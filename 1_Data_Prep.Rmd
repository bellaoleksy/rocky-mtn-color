---
title: "Colorado Mountain Lake Color Trends/Controls"
author: "Matthew Ross"
date: "01/12/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---



```{r setup, include=FALSE}
library(tidyverse)
library(leaflet)
library(sf)
library(feather)
library(lubridate)
library(mapview)
library(USAboundaries)
library(s2)
library(elevatr)
library(trend)
library(tsibble)
library(imputeTS)
library(ggthemes)
library(nhdplusTools)

sf_use_s2(use_s2 = T)
knitr::opts_chunk$set(echo = TRUE, warning = F, comment = F, message = F)
```

#  LimnoSat-US Download and Filtering

This document started out as a tutorial by Simon Topp for working with LimnoSat,
a dataset of Landsat lake color. To work with this data, we first need to download
it in the chunk below. 

```{r LimnoSat Download, eval = F}
## Pull the URLS from the zenodo repo. More information on the contents of these files can be found at
## https://doi.org/10.5281/zenodo.4139694.
ls.urls <- httr::GET("https://zenodo.org/api/records/4139694") 
ls.urls <- jsonlite::fromJSON(httr::content(ls.urls, as = "text"))
files <- ls.urls$files
urls <- files$links$download

## Identify/Create the folder you want to store the data in
folder <- 'data/LimnoSat'
if (file.exists(folder)){
  folder <- paste0(folder,'/')
} else {
    dir.create(folder)
  folder <- paste0(folder,'/')
}

##Download the Deepest point shapefile.  This contains the locations of all the lakes in the database.
## Note: On windows you need mode = 'wb' over the default mode = 'w' for download.file)
grep('DP', urls, value = T) %>% purrr::map(., ~download.file(., paste0(folder,basename(.)), mode = 'wb'))

## Download the scene metadata.  This includes things like scene cloud cover and sun angle for all the 
## remote sensing observations in LimnoSat-US.
meta.url <- grep('SceneMetadata', urls, value = T)
download.file(meta.url, paste0(folder,basename(meta.url)), mode = 'wb')

## Download the actual LimnoSat Database, here we'll download the .feather version because it's
## a little smaller, if you'd prefer the csv, just swap out the .feather with .csv below
## Note: This takes  a couple minutes because the file is ~3gb. Also, we'll rename the file to be
## more user friendly.
ls.url <- grep('srCorrected_us_hydrolakes_dp_20200628.feather', urls, value = T)
download.file(ls.url, paste0(folder, 'LimnoSat_20200628.feather'), mode = 'wb')

rm(ls.url, ls.urls, meta.url, urls, files,)
```

## Subset Limnosat to Intermountain West and Join with NHD



```{r, eval = F, warning = F}
## Read in the geospatial data
# Lakes
lakes <- st_read('data/LimnoSat/HydroLakes_DP.shp') %>%
  st_centroid() %>%
  filter(type == 'dp') # Grab only deepest point data

#Get colorado outline
co <- us_states() %>%
  filter(state_abbr %in% c('CO','WY','MT','ID','UT','NM')) %>%
  st_transform(st_crs(lakes))

# With elevation data
co_lakes <- lakes[co,]  %>%
  get_elev_point(.)

## FUI - Create Forel-Ule Color table

#Connect dWL to the forel ule index for visualization
fui.lookup <- tibble(dWL = c(471:583), fui = NA)
fui.lookup$fui[fui.lookup$dWL <= 583] = 21
fui.lookup$fui[fui.lookup$dWL <= 581] = 20
fui.lookup$fui[fui.lookup$dWL <= 579] = 19
fui.lookup$fui[fui.lookup$dWL <= 577] = 18
fui.lookup$fui[fui.lookup$dWL <= 575] = 17
fui.lookup$fui[fui.lookup$dWL <= 573] = 16
fui.lookup$fui[fui.lookup$dWL <= 571] = 15
fui.lookup$fui[fui.lookup$dWL <= 570] = 14
fui.lookup$fui[fui.lookup$dWL <= 569] = 13
fui.lookup$fui[fui.lookup$dWL <= 568] = 12
fui.lookup$fui[fui.lookup$dWL <= 567] = 11
fui.lookup$fui[fui.lookup$dWL <= 564] = 10
fui.lookup$fui[fui.lookup$dWL <= 559] = 9
fui.lookup$fui[fui.lookup$dWL <= 549] = 8
fui.lookup$fui[fui.lookup$dWL <= 530] = 7
fui.lookup$fui[fui.lookup$dWL <= 509] = 6
fui.lookup$fui[fui.lookup$dWL <= 495] = 5
fui.lookup$fui[fui.lookup$dWL <= 489] = 4
fui.lookup$fui[fui.lookup$dWL <= 485] = 3
fui.lookup$fui[fui.lookup$dWL <= 480] = 2
fui.lookup$fui[fui.lookup$dWL <= 475 & fui.lookup$dWL >470] = 1


# Actual Forel-Ule Colors
fui.colors <- tibble(color = c(
  "#2158bc", "#316dc5", "#327cbb", "#4b80a0", "#568f96", "#6d9298", "#698c86", 
  "#759e72", "#7ba654", "#7dae38", "#94b660","#94b660", "#a5bc76", "#aab86d", 
  "#adb55f", "#a8a965", "#ae9f5c", "#b3a053", "#af8a44", "#a46905", "#9f4d04"),
  fui = 1:21)

#LimnoSat color information
ls_co <- read_feather('data/LimnoSat/LimnoSat_20200628.feather') %>%
  filter(Hylak_id %in% co_lakes$Hylak_id) %>%
  inner_join(co_lakes %>% 
               as.data.frame(.) %>%
               select(-geometry) %>%
               select(Hylak_id,elevation)) %>%
  left_join(fui.lookup) %>%
  left_join(fui.colors)





save(ls_co, co_lakes, fui.lookup, fui.colors, file = 'data/ls_elev.RData')
```


I blame COVID, but I can't think through how to do this data prep only once.
The populations of lakes for the median modern distribution and the trend lakes
are different, so they need to be prepped twice, which feels wasteful and insane,
but not sure how else to do it. 


# Blue vs. Green Lake Distributions


## Modern Blue/Green Data Prep


```{r}
load('data/ls_elev.RData')

#Grab modern decade median color
co_summary_median <- ls_co %>%
  dplyr::select(-year, -sat) %>%
  mutate(month = month(date),
         year = year(date),
         doy = yday(date)) %>%
  filter(month %in% 7:9, 
         doy <= 258, #Sept 14/15
         year >= 2009,
         elevation > 1400) %>% 
  group_by(Hylak_id) %>%
  add_count() %>%
  filter(n >= 30) %>%
  group_by(Hylak_id) %>%
  summarize(across(c(Blue:dWL),
                   .fns = list(mean = mean, 
                               sd = sd, 
                               max = max,
                               min = min,
                               median = median), na.rm = T,
                   .names = "{.col}-{.fn}"))%>% 
  ungroup() %>%
  pivot_longer(cols = `Blue-mean`:`dWL-median`,
               names_to = c('var','stat'),
               names_sep = '-')

```


### Join to NHD features

```{r, eval = F}

likely_lakes <- co_lakes %>%
  filter(Hylak_id %in% co_summary_median$Hylak_id) 


singular_fetch <- function(index = 1){
  
  wbd <- try(get_waterbodies(likely_lakes[index,]), silent = T)
  
  if(length(class(wbd)) == 1){
    wbd <- NULL
  } else {
    wbd <- wbd %>%
      dplyr::select(id:onoffnet,meandepth,maxdepth) %>%
      mutate(onoffnet = as.character(onoffnet),
             meandepth = as.numeric(meandepth),
             maxdepth = as.numeric(maxdepth))
  }
  
  return(wbd)
}

full_fetch <- map(1:nrow(likely_lakes),singular_fetch)



full_nhd <- do.call('rbind',full_fetch)


definitely_lakes <- likely_lakes[full_nhd,] 



nhd_hylak_duplicated <- st_join(definitely_lakes,
                     full_nhd %>%
                       dplyr::select(-elevation))
#Remove lakes that join to multiple NHDs. Impossible to resolve which numbers to take
#from lake cat

nhd_dupes <- nhd_hylak_duplicated %>%
  as.data.frame() %>%
  select(-geometry) %>%
  count(Hylak_id) %>%
  filter(n > 1)

nhd_hylak <- nhd_hylak_duplicated %>%
  filter(!Hylak_id %in% nhd_dupes$Hylak_id)

save(full_nhd,nhd_hylak, file = 'data/nhd_lakes.RData')
```



### Join to Lagos Res and LakeCAT

- LakeCat data is still corrupted. 


# BELLA WORK HERE!

```{r}


rsrvr <- read_csv('data/LAGOSUS_RSVR_v1.1.csv') %>%
  filter(lake_centroidstate %in% c('CO','UT','WY','MT','NM',
                                   'ID','NV')) %>%
  dplyr::select(lagoslakeid, lake_rsvr_class)

lake_link <- read_csv('data/lake_link.csv') %>%
  dplyr::filter(!is.na(nhdplusv2_comid)) %>%
  distinct(lagoslakeid, nhdplusv2_comid, .keep_all = T) %>%
  filter(lagoslakeid %in% rsrvr$lagoslakeid) %>%
  inner_join(rsrvr)



############### BELLA THIS IS STILL FROM THE OLD DATA ###################
lakeCat <- read_feather('data/lakeCat.feather') %>%
  # A billion dupes, why? 
  distinct(nhdplusv2_comid, lagoslakeid,.keep_all = T) %>%
  select(-lake_nhdid,-lake_lat_decdeg,-lake_lon_decdeg)



### LInking above through lagos lake_link
lake_descriptor <- nhd_hylak %>%
  inner_join(lake_link, by = c('comid' = 'nhdplusv2_comid')) %>%
  inner_join(lakeCat) %>%
  #These are truly identical dupes and I have no clue how they keep sneaking through
  # distinct is dangerous but appropriate here. 
  distinct(Hylak_id,.keep_all = T)



## OLd code that justifies the above if you remove `distinct(Hylak_id)` you can
# see why
#Sneaky dupes
# descriptor_dupes <- lake_descriptor %>%
#   as.data.frame() %>%
#   select(-geometry) %>%
#   count(Hylak_id) %>%
#   filter(n > 1)
# 
# 
# #Map dupes
# sneaky_dupes <- lake_descriptor %>%
#   filter(Hylak_id %in% descriptor_dupes$Hylak_id) 
# 
# View(sneaky_dupes)
# 
# View(descriptor_dupes)

```



### Meyer et al., GLCP

```{r, eval = F}

names(glcp_high_lakes)
glcp <- read_csv('data/glcp.csv')

glcp_high_lakes <- glcp |> 
  filter(Hylak_id %in% nhd_hylak$Hylak_id)

save(glcp_high_lakes, file = 'data/glcp_sub.RData')
rm(glcp)

gc()
```



