---
title: "LimnoSat-US_Tutorial"
author: "Simon Topp"
date: "11/2/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---




```{r setup, include=FALSE}
library(tidyverse)
library(leaflet)
library(sf)
library(feather)
library(lubridate)
library(mapview)
library(USAboundaries)
library(s2)
library(elevatr)
library(trend)
library(tsibble)
library(imputeTS)
library(ggthemes)
library(nhdplusTools)
library(tmap)
library(raster)
library(caret)
library(rpart)
library(yardstick)
library(rpart.plot)
library(patchwork)
library(cowplot)
library(lfstat) #for water_year function
library(zyp) #for zyp::zyp.sen, getting intercept for plotting sens slopes


sf_use_s2(use_s2 = T)
knitr::opts_chunk$set(echo = TRUE, warning = F, comment = F, message = F)
```


## Trend Analysis

### Trend Data prep

Lots of key decisions in here. Could iterate


```{r, eval = F}



co_summary <- ls_co %>%
  dplyr::select(-year, -sat) %>%
  mutate(month = month(date),
         year = year(date),
         doy = yday(date)) %>%
  filter(month %in% 7:9, 
         doy <= 258) %>% #Sept 14/15
  group_by(Hylak_id,year) %>%
  add_count() %>%
  filter(year >= 1984) %>% #changed to >= so we don't drop 1984... 
  filter(n >= 3) %>% # at least 3 observations per year
  group_by(Hylak_id) %>%
  mutate(unique_years = n_distinct(year)) %>%
  filter(unique_years > 30) %>% # at least 30 years of data
  arrange(year) %>%
  #complete years
  group_by(Hylak_id,year) %>%
  summarize(across(c(Blue:dWL),
                   .fns = list(mean = mean,
                               sd = sd,
                               max = max,
                               min = min,
                               median = median), na.rm = T,
                   .names = "{.col}-{.fn}"))%>%
  ungroup() %>%
  as_tsibble(., key = Hylak_id, index=year) %>% #time series tibble
  fill_gaps() %>% #imputing, though not sure what's going on under the hood
  as_tibble() %>%
  pivot_longer(cols = `Blue-mean`:`dWL-median`,
               names_to = c('var','stat'),
               names_sep = '-') %>%
  group_by(Hylak_id, var, stat) %>%
  arrange(year) %>%
  mutate(impute_value = na_interpolation(value, maxgap = 1))


filled_enough <- co_summary %>%
  group_by(Hylak_id) %>%
  summarize(any_nas = any(is.na(impute_value))) %>% # check if there are missing values in each Hylak_id timeseries
  filter(any_nas == F) # only select the sites with a max of 1 year of missing data


co_full <- co_summary %>%
  filter(Hylak_id %in% filled_enough$Hylak_id) 

#How many lakes? 
length(unique(co_full$Hylak_id))

save(co_full, file = 'data/color_summary.RData')
```


## PRISM

#### Get lat/long for PRISM
```{r}
#Extract lat/long for PRISM
dwl_all_ungrouped<-dwl_all%>%ungroup()%>%select(Hylak_id, geometry) 

separated_coord <- dwl_all_ungrouped %>%
    mutate(lat = unlist(map(dwl_all_ungrouped$geometry,2)),
           long = unlist(map(dwl_all_ungrouped$geometry,1))) %>%
  select(-geometry) %>%
  distinct() %>%
  relocate(Hylak_id, .after = last_col())

#PRISM can only pull 500 sites at a time, so split into two dataframes
separated_coord_1<-separated_coord %>%
  slice(1:500)
separated_coord_2<-separated_coord %>%
  slice(501:nrow(.))
# write_csv(separated_coord_1, "data/prism/separated_coord_1.csv", col_names = FALSE)
# write_csv(separated_coord_2, "data/prism/separated_coord_2.csv",col_names = FALSE)
# Use the above .csv files to download PRISM data for each individual lakes
# https://prism.oregonstate.edu/explorer/bulk.php 

#Read in PRISM data
dir<-here("data/prism/download")
files<-list.files(dir)
prism<-data.frame() # data frame to store all prism data
for(i in 1:length(files)){ # loops over all files in prism/download directory
  cur<-read.table(file.path(dir,paste(files[i],sep='')),
                  header=T,sep=",",skip=10,
                  stringsAsFactors = F) # read in all prism files
  cur$fileName<-files[i]
  #keep track of which .csv file data came from to make sure it's all there
  prism<-rbind(prism,cur)
}
prism<-prism %>%
    rename(ppt_mm=`ppt..mm.`,
           tmean_degC=`tmean..degrees.C.`,
           elev_m_prism=`Elevation..m.`,
           Hylak_id=Name,
           lon_dec_deg=Longitude,
           lat_dec_deg=Latitude,
           date=Date)%>%
  mutate(date=ym(date)) 
  # select(-fileName)

# Double check that all files joined
# length(unique(prism$fileName))

#Double check to make sure all data are there
str(prism)


```

### Summary stats
```{r}


# Summary statistics by site
prism_sum <- prism %>%
  select(-c(lat_dec_deg,lon_dec_deg,elev_m_prism,fileName)) %>%
  pivot_longer(-c(Hylak_id, date)) %>%
  mutate(year=year(date),
         water_year=water_year(date, "usgs"), #uses the USGS standard of Oct 
         month=month(date),
         season = case_when(year==water_year & month %in% c(3,4,5) ~ "spring",
                           year==water_year & month %in% c(6,7,8) ~ "summer",
                           year==water_year & month %in% c(1,2) ~ "winter",
                           year!=water_year & month %in% c(12) ~ "winter"),
         # Hylak_id=as.factor(as.integer(Hylak_id)),
         water_year=as.numeric(as.character(water_year))) %>%
  drop_na(season)%>%
  group_by(Hylak_id, water_year, season, name) %>%
  summarize(mean=mean(value, na.rm=TRUE))

prism_sum_ppt<-prism_sum %>%
  filter(name=="ppt_mm")
prism_sum_temp<-prism_sum %>%
  filter(name=="tmean_degC")

##########################################################################################
##Functions for calculating sens slopes and intercepts for plotting later
##########################################################################################
map_sens2 <- function(df){
  sens.slope(df$mean)
}


sens_slope <- function(mod){
  mod$estimate
}

#https://kevintshoemaker.github.io/NRES-746/TimeSeries_all.html
#For getting sens intercept, helpful for plotting later
map_zyp <- function(df){
  zyp::zyp.sen(mean~water_year,df)
  # sens.slope(df$mean)
}


sens_intercept <- function(mod){
  mod$coefficients[[1]] # pull out y-int estimate for ploting
}


#Coefficient of variation -- how variation is temp and precip over time? 
cv <- function(df)  {
  abs(sd(df$mean)/mean(df$mean))
}

## Precipitation first--- 
## Only keeping these separate for the trend categories.
prism_sum_ppt_nested<-prism_sum_ppt %>%
    group_by(Hylak_id, season, name) %>%
    nest() %>%
    mutate(sens = map(data, map_sens2),
           sens_sum = map(sens,broom::glance),
           slope = map(sens,sens_slope),
           zyp_mod = map(data,map_zyp),
           cv = map(data,cv),
           intercept = map(zyp_mod,sens_intercept))

prism_sum_temp_nested<-prism_sum_temp %>%
    group_by(Hylak_id, season, name) %>%
    nest() %>%
    mutate(sens = map(data, map_sens2),
           sens_sum = map(sens,broom::glance),
           slope = map(sens,sens_slope),
           zyp_mod = map(data,map_zyp),
           cv = map(data,cv),
           intercept = map(zyp_mod,sens_intercept))

## Un-nest PPT
prism_sum_ppt_unnested = unnest(prism_sum_ppt_nested, c(sens_sum,slope, season, name))%>%
    mutate(Trend_ppt = case_when(p.value <= 0.05 & slope >= 0 ~ 'wetter',
                             p.value <= 0.05 & slope <= 0 ~ 'drier',
                             p.value > 0.05 ~ 'No trend'),
           Trend_ppt = factor(Trend_ppt, 
                          levels = c('No trend',
                                      'wetter',
                                     'drier'))) 
## Un-nest TEMP
prism_sum_temp_unnested = unnest(prism_sum_temp_nested, c(sens_sum,slope, season, name))%>%
    mutate(Trend_temp = case_when(p.value <= 0.05 & slope >= 0 ~ 'warmer',
                             p.value <= 0.05 & slope <= 0 ~ 'cooler',
                             p.value > 0.05 ~ 'No trend'),
           Trend_temp = factor(Trend_temp, 
                          levels = c('No trend',
                                      'warmer',
                                     'cooler'))) 

#Extract all covariates for modeling...
prism_ppt_trends_wide<-prism_sum_ppt_unnested %>%
  select(Hylak_id, season, name, cv, slope)%>%
  unnest(c(cv, slope)) %>%
  pivot_wider(names_from=c("season","name"),
                values_from=c("slope","cv"))

prism_temp_trends_wide<-prism_sum_temp_unnested %>%
  select(Hylak_id, season, name, cv, slope) %>%
  unnest(c(cv, slope)) %>%
  pivot_wider(names_from=c("season","name"),
              values_from=c("slope","cv"))


##Dataframe with all the actual slopes and cv values
prism_trends_wide<-inner_join(prism_ppt_trends_wide,prism_temp_trends_wide)

##Data with all the ppt and temp trend categories
ppt_trend_categories<-prism_sum_ppt_unnested%>%
  select(Hylak_id,Trend_ppt) %>%
  pivot_wider(names_from=c("season"),
              values_from=c("Trend_ppt"),
              names_prefix="ppt_trend_") %>%
  ungroup()%>%
  select(-name)
temp_trend_categories<-prism_sum_temp_unnested%>%
  select(Hylak_id,Trend_temp) %>%
  pivot_wider(names_from=c("season"),
              values_from=c("Trend_temp"),
              names_prefix="temp_trend_") %>%
  ungroup()%>%
  select(-name)
prism_trend_categories<-inner_join(ppt_trend_categories,temp_trend_categories)
```

#### Interrogate trends
```{r}

#How many lakes are getting wetter/drier in each season?
prism_sum_ppt_unnested %>%
  unnest(data) %>%
  ungroup() %>%
  mutate(n_lakes_total=length(unique(Hylak_id))) %>%
  group_by(Trend, season, n_lakes_total) %>%
  summarize(n_trending=length(unique(Hylak_id))) %>%
  mutate(perc=(n_trending/sum(n_lakes_total))*100,
         perc=round(perc,1)) 

#How many lakes are getting warmer/cooler in each season?
prism_sum_temp_unnested %>%
  unnest(data) %>%
  ungroup() %>%
  mutate(n_lakes_total=length(unique(Hylak_id))) %>%
  group_by(Trend, season, n_lakes_total) %>%
  summarize(n_trending=length(unique(Hylak_id))) %>%
  mutate(perc=(n_trending/sum(n_lakes_total))*100,
         perc=round(perc,1)) 

```

## Human populations trends
```{r}
population_trends<-  glcp_high_lakes %>%
  select(Hylak_id, year,pop_sum) %>%
  drop_na() %>%
  pivot_wider(names_from="year",
              values_from="pop_sum",
              names_prefix="pop_year_") %>%
  group_by(Hylak_id) %>%
  summarize(population_change=((pop_year_2015-pop_year_1995)/pop_year_1995)*100)

```


## Net Trends in DWL


```{r}

load('data/ls_elev.RData')
load('data/color_summary.RData')
load('data/nhd_lakes.RData')
load('data/glcp_sub.RData')

nhd_hylak <- nhd_hylak %>%
  filter(elevation >= 1400) %>% ## changed to have the same cut off as in 2_GreenBlueMedians.Rmd? 
  filter(ftype %in% c('LakePond','Reservoir')) 

actually_high_lakes <- nhd_hylak


high_lakes <- co_full %>%
  filter(Hylak_id %in% actually_high_lakes$Hylak_id)

#How many lakes? 
length(unique(high_lakes$Hylak_id))


map_sens <- function(df){
  sens.slope(df$impute_value)
}


#https://kevintshoemaker.github.io/NRES-746/TimeSeries_all.html
#For getting sens intercept, helpful for plotting later
map_zyp <- function(df){
  zyp::zyp.sen(impute_value~year,df)
}


sens_intercept <- function(mod){
  mod$coefficients[[1]] # pull out y-int estimate for ploting
}


sens_est <- function(mod){
  mod$estimate
}

val_mean <- function(df){
  early_mean <- df %>%
    filter(year < 2005) %>%
    pull(impute_value) %>%
    mean(., na.rm = T) 
}

trend_plotter <- function(study_lakes = high_lakes,
                          v = 'dWL',
                          x = 'Dominant Wavelength Sens Slope'){
  
  
  trend_plot <- function(df){
    ggplot(df,aes(x = year,
                  y = impute_value)) +
      geom_point() + 
      stat_smooth(method = 'lm')
  }
  
  
  co_mods <- study_lakes %>%
    filter(var == v) %>%
    group_by(Hylak_id, stat) %>%
    nest() %>%
    mutate(sens = map(data, map_sens),
           plots = map(data, trend_plot),
           sens_sum = map(sens,broom::glance),
           slope = map(sens,sens_est),
           zyp_mod = map(data,map_zyp),
           intercept = map(zyp_mod,sens_intercept),
           early_mean = map(data, val_mean))
  
  
            
           
  
  
  sens_sum = unnest(co_mods, c(sens_sum,slope, early_mean)) %>%
    mutate(Trend = case_when(p.value <= 0.05 & slope >= 0 & early_mean < 530 ~ 'Blue -> Green',
                             p.value <= 0.05 & slope >= 0 & early_mean >= 530 ~ 'Intensifying Green/Yellow', #Added >= instead of > IAO 2022-01-25
                             p.value <= 0.05 & slope <= 0 & early_mean > 530 ~ 'Green -> Blue',
                             p.value <= 0.05 & slope <= 0 & early_mean < 530 ~ 'Intensifying Blue',
                             p.value > 0.05 ~ 'No trend'),
           Trend = factor(Trend, 
                          levels = c('No trend',
                                      'Intensifying Blue',
                                     'Green -> Blue',
                                     'Intensifying Green/Yellow',
                                     'Blue -> Green')))
  

  plot <- ggplot(sens_sum %>%
                  dplyr::filter(stat != 'sd'),aes(x=slope,color=Trend)) + 
    geom_freqpoly(bins = 20, size = 1) + 
    facet_wrap(~stat) +
    theme_few() + 
    theme(legend.position = 'top',
          legend.direction = 'horizontal') +
    guides(color=guide_legend(nrow=2, byrow=TRUE)) + 
    scale_color_manual(values = c('gray20','#1f78b4','#a6cee3','#b2df8a','#33a02c'),
                       name = '') + 
    xlab(x) + 
    ylab('Lake count')
  
  
  return(list(sens_sum, plot))
}
```

## DWL modes

### DWL histogram


```{r}
high_lakes_descriptor <- glcp_high_lakes %>%
  group_by(Hylak_id) %>%
  summarize(across(where(is.numeric),mean))

 

high_modes <- high_lakes %>%
  filter(var == 'dWL',
         stat == 'median') %>%
  dplyr::mutate(decade = cut(year, breaks = c(1984,1996,2008,2021),
                             labels = c('1985-96','1997-2008','2009-2020'))) %>%
  group_by(Hylak_id, decade) %>%
  summarize(dWL = round(median(value, na.rm = T),0)) %>%
  ungroup() %>%
  inner_join(fui.lookup) %>%
  inner_join(fui.colors) 


bg.fui = tibble(
  ymin = c(470,475,480,485,489,495,509,530,549,559,564,567,568,569,570,573,575,577,579,581,583),
  ymax = c(475,480,485,489,495,509,530,549,559,564,567,568,569,570,573,575,577,579,581,583,590),
  color = c(
  "#2158bc", "#316dc5", "#327cbb", "#4b80a0", "#568f96", "#6d9298", "#698c86", 
  "#759e72", "#7ba654", "#7dae38", "#94b660","#94b660", "#a5bc76", "#aab86d", 
  "#adb55f", "#a8a965", "#ae9f5c", "#b3a053", "#af8a44", "#a46905", "#9f4d04")
)

##Legend only
legend<-ggplot() + 
  geom_rect(data = bg.fui, 
            aes(ymin = ymin, ymax = ymax, xmin = -5, xmax = 100, fill = color)) + 
  scale_fill_identity() + 
  theme_few() +
  scale_x_continuous(expand = c(0, 0))+
  scale_y_continuous(expand = c(0, 0))+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_text(size=5),
        axis.title.y=element_text(size=6),
        plot.title=element_text(size=6, hjust=-0.05))+
  labs(y="Dominant wavelength (nm)",
       title="Forel-Ule\ncolor")

# make prettier
# png(filename = 'figs/dwl_hist_three_periods.png', width = 4, height = 4,
#     units = 'in', res = 300)
# 
# 
# base<-ggplot() + 
#   facet_wrap(~decade, ncol = 1) + 
#   geom_rect(data = bg.fui, 
#             aes(xmin = ymin, xmax = ymax, ymin = -5, ymax = 100, fill = color)) + 
#   geom_vline(xintercept = 530, linetype = 3) +
#   geom_histogram(data = high_modes, aes(x=dWL, y = ..density..), fill = NA,
#                  color = 'black', alpha = .3, bins = 40) + 
#   geom_density(data = high_modes, aes(x=dWL)) + 
#   scale_x_continuous(expand = c(0, 0))+ #get rid of whitespace
#   scale_fill_identity() + 
#   theme_few() +
#   coord_cartesian(ylim = c(0,.03))  +
#   labs(x="Dominant wavelength (nm)",
#        y="Density")
# 
# base+inset_element(legend, 0.85, 0.75, 1, 1)
# 
# layout <- '
# A#
# AB
# A#
# '
# wrap_plots(A = base, B = legend, design = layout)+
#   plot_layout(widths = c(5, 0.5))
# 
# 
# dev.off()





```


## DWL Trends

```{r}
dWL_trends <- trend_plotter()

##IAO-- this is cool but I think we will want to only look at median.
#Or if we want to look at max, maybe filter down to only sites with >3 observations a year?

# png(filename = 'figs/dwl_hist.png', width = 4,
#     height = , units = 'in', res = 300)
# dWL_trends[[2]]
# dev.off()

## Pull data for plotting

dwl <- dWL_trends[[1]] %>%
  inner_join(nhd_hylak, by = 'Hylak_id')

dwl_all <- dwl %>%
  filter(stat == 'median') %>%
  unnest(data) %>%
  unnest(intercept)


# set.seed(111)
# set.seed(1)
set.seed(999)
dwl_specials <- dwl_all %>%
  # filter(abs(slope) >= 1 | Trend == 'No trend') %>%
  # filter(p.value < 0.05 | Trend == 'No trend') %>% # Had to change from 0.001 to 0.05 to get a blue->green example lake
  filter(abs(slope) >= 1 & p.value < 0.05) %>% #I think it would be better to not show a trend line for "No Trend"
  group_by(Trend) %>%
  sample_n(1) %>%
  pull(Hylak_id) 

full_spec <- dwl_all %>%
  filter(Hylak_id %in% dwl_specials)

#How many lakes in each trend category?
trend_labels<-dwl_all %>%
  group_by(Trend) %>%
  summarize(n=length(unique(Hylak_id))) %>%
  mutate(perc=(n/sum(n))*100,
         perc=round(perc,0)) 


trendColors <- c('No trend'='grey90',
                'Intensifying Blue'='#1f78b4',
                'Green -> Blue'='#a6cee3',
                'Intensifying Green/Yellow'='#33a02c',
                'Blue -> Green'='#b2df8a')


#Export
png(filename = 'figs/Trend Examples.png',
    width = 5,
    height = 8,
    res = 600,
    units = 'in')
ggplot(dwl_all, aes(x = year, y = value, group = Hylak_id)) +
  geom_abline(aes(intercept = intercept, slope = slope), color="grey50", size=0.1, alpha=0.5) + #background Sen's slopes
  geom_abline(data = full_spec, 
              aes(intercept = intercept, slope = slope, color=Trend),
              size=1) + #Sen's slope for selected sites
  geom_point(data = full_spec, aes(fill=Trend), shape=21) + 
  scale_fill_manual(values=trendColors)+
  scale_color_manual(values=trendColors)+
  geom_text(x = 1990, y = 585,
            aes(group=Trend, label = paste("n =",n)),
            color="black",
            data = trend_labels, size=3)+ #Add label for sample size
  scale_x_continuous(breaks=seq(1986, 2018, 6))+
  facet_wrap(~Trend, nrow=5) + 
  theme_few()+
  theme(legend.position="none",
        plot.margin = unit(c(0,0.2,0,0), "cm"),)+
  labs(y="Dominant wavelength (nm)",
       x="Year") +
  
  ggplot(dwl_all %>%
             distinct(Hylak_id,.keep_all = T),aes(x=slope,color=Trend)) + 
  geom_freqpoly(bins = 20, size = 1) + 
  geom_vline(xintercept=0, linetype="dashed", size=0.5)+
  facet_wrap(~Trend, nrow=5, scales="free_y") +
  theme_few() + 
  scale_y_continuous(position="right")+
  scale_x_continuous(breaks=seq(-1.5, 1.5, 1))+
  theme(legend.position="none",
        plot.margin = unit(c(0,0,0,0.2), "cm"),
        strip.text.x = element_text(color="white"), #Keep strips but color white so they are invisible
        legend.direction = 'horizontal') +
  scale_color_manual(values=trendColors)+
  xlab("Slope")+
  ylab('Lake count') + 
  plot_layout(widths = c(1, 0.6))
dev.off()

```



## DWL map

```{r}

class(definitely_lakes)
dwl_map <- dwl %>%
  filter(stat == 'median') %>%
  dplyr::select(Hylak_id,Trend) %>%
  inner_join(site_cols,.) %>%
  arrange(Trend)



png(filename = 'figs/trend_map.png',
    width = 4, height = 6, units = 'in', res = 300)

tm_shape(hil) +
  tm_raster(palette = gray(0:10 / 10), style = "cont", legend.show = FALSE) + 
tm_shape(ras) + 
  tm_raster(alpha = 0.4,
            style = 'cont',
            title = 'Elevation (m)',
            palette = 'Greys',
            legend.show = F) + 
tm_shape(states) + 
  tm_borders(lwd = 1,
             col = 'black') + 
tm_shape(dwl_map) + 
    tm_bubbles(size = .05,
             col = 'Trend',
             shape=21,
             border.col = 'black',
             border.lwd = 1,
             # jitter = .3,
             palette = c('gray20','#1f78b4','#a6cee3','#33a02c','#b2df8a')) +
tm_legend(legend.position = c(0,0),
            legend.bg.color = 'gray90',
            legend.frame = 'black') 
dev.off()
```


## DWL Trends -- why?
```{r}

#Create df for CART modeling

trends_df<-dwl_all %>%
  ungroup()%>%
  filter(Trend!="No trend") %>%
  select(Hylak_id, Trend) %>%
  distinct(Hylak_id,.keep_all = T) %>%
  # inner_join(blue_green_2018) %>% #add LakeCat variables
  inner_join(prism_trends_wide) %>% #add PRISM variables
  inner_join(population_trends) %>% #add GLCP population variables
  inner_join(prism_trend_categories) %>% #prism trend categories
  # select(-c(air_temp,precip,pop_sum)) %>%
  select(-c(contains(c("perc_","cv_"))))

names(trends_df)

set.seed(4444)

train_d <- trends_df %>%
  sample_frac(0.8) 

test_d <- trends_df %>%
  dplyr::filter(!Hylak_id %in% train_d$Hylak_id) %>%
  select(-Hylak_id) %>%
  mutate_if(is.numeric, round, digits=2) 
  # dplyr::select(-Hylak_id,-dWL, -group)

train_d <- train_d %>%
  select(-Hylak_id) %>%
  mutate_if(is.numeric, round, digits=2) 
  # dplyr::select(-Hylak_id,-dWL, -group)



## Not run, but code that helped make decision about complexity parameter
# prune_mod <-caret::train(
#   group ~., data = train_d, method = "rpart",
#   trControl = trainControl("boot", number = 100),
#   tuneLength = 100
#   )



cart_mod <- rpart(Trend ~ ., data = train_d,
                  method = 'class',
                  cp = 0.03) # CP is a tuning knob for tree complexity.



test_d$guess <- predict(cart_mod, test_d, 'class')
cm <- conf_mat(test_d, Trend,guess)
accuracy(test_d,Trend,guess)

# png(filename = 'figs/bg_tree.png',
# width = 7, height = 8, units = 'in', res = 300)
rpart.plot(cart_mod,
           type = 2, #2=split labels below node labels; but I like the look of 4 with labels on both connecting lines and no "yesno"
           extra = 2,#2 = class rate; 106 = prob. of 2nd class + percent of observations
           under = TRUE, #I kind of like the look of this better
           clip.right.labs = FALSE,
           branch = 0.3,
           # branch.type = 5, yesno=FALSE, #If you comment out type=4, I kind of like the look of this
           branch.col = "gray",
           # main="Factors that influence lake category",
           box.palette = list('#1f78b4','#a6cee3','#33a02c','#b2df8a'))
# dev.off()

## Create manually because the plot_confusion_matrix function is pissing me off
conf_mat <- confusion_matrix(targets = test_d$Trend,
                             predictions = test_d$guess)
table<-data.frame(conf_mat$`Confusion Matrix`)

plotTable <- table %>%
  group_by(Target) %>%
  mutate(prop = N/sum(N),
         prop = round(prop,2))

# png(filename = 'figs/bg_confusion_matrix_gg.png',
#     width = 6, height = 5,
#     units = 'in', res = 300)
ggplot(data = plotTable,
       mapping = aes(x = Target, y = Prediction)) +
  # geom_tile(aes(fill=Pos_Blue.clear), color="black") +
  # scale_x_discrete(expand = c(0, 0))+ #remove white space
  # scale_y_discrete(expand = c(0, 0))+ #remove white space
  geom_text(aes(label = paste0("n=",N)), vjust = .5,  alpha = 1, size=4) +
  geom_text(aes(label = paste0("prop.=",prop)), vjust = 2.0,  alpha = 1, size=3) +
  scale_fill_manual(values = c(TP = "#1f78b4", TN = "#33a02c",
                               FN = "white" , FP ="white")) +
  theme_few() +
  # xlim(rev(levels(table$Target)))+
  theme(legend.position="none")
# dev.off()



##GGPARTY? Another way
library(ggparty)
cart_mod_party<-as.party(cart_mod)
# plot(cart_mod_party)
ggparty(cart_mod_party) +
  geom_edge(size=1, color="grey50") +
    # geom_node_label(aes(label = splitvar), ids = "inner") +
  geom_edge_label() +
  geom_node_splitvar() +
  geom_node_plot(gglist = list(geom_bar(aes(x = "", fill = Trend),
                                    position = position_fill()),#standardizes so each stack has constant height; plots proportions.
                               (scale_fill_manual(values=trendColors)),
                                                               (theme_few()),
                               (theme(plot.margin = unit(c(0,0.2,0,0.2), "cm"),
                                axis.title.x=element_blank(),
                                axis.text.x=element_blank(),
                                axis.ticks.x=element_blank(),
                                legend.position="none")),
                               (scale_x_discrete(expand = c(0, 0))), #remove white space
                               # (scale_y_discrete(expand = c(0, 0))), #remove white space)
                               ylab("Proportion")),
                               shared_axis_labels = TRUE) +
  theme(plot.margin = unit(c(0,0,0,0), "cm"))

## "Your decision tree may be cool, but what if I tell you you can make it hot?" LMAO.
# https://trang1618.github.io/treeheatr/articles/explore.html
# library(treeheatr)
# 
# # build tree using rpart:
# x <- partykit::as.party(cart_mod)
# 
# tree_plot<-heat_tree(x,
#           target_lab = 'group',
#           target_cols = c('#1f78b4','#33a02c'),
#           feats = NA, 
#           # feats = c('slope', 'maxdepth', 'WALA', 'air_temp'), #select only features on the tree
#           # show = 'tree-only',
#           # cont_legend = TRUE, cate_legend = TRUE,
#           panel_space = 0.05, target_space = 0.2, tree_space_bottom = 0.1, heat_rel_height = 0.1,
#           edge_vars = list(size = 1, color = 'grey'))
# tree_plot


#Combine
library(patchwork)
# (tree_plot + conf_matrix_plot + spatial_hist )+
#   plot_layout(ncol = 1) 
# 
# spatial_hist / (tree_plot + conf_matrix_plot + plot_layout(widths = c(1,0.5))) + 
#   plot_layout(heights = c(0.5,1.5))

##Arrangement 1
png(filename = 'figs/spatial_plots.png',
width = 6, height = 7, units = 'in', res = 600)
C<-(spatial_hist+conf_matrix_plot)
(C/tree_plot)+ plot_layout(heights = c(0.3,1))+  plot_annotation(tag_levels = 'A')
#For top right alginment: inset_element(conf_matrix_plot, left = 0.67, bottom = 0.67, right = 1, top = 1, align_to = 'full')) 
dev.off()

##Arrangement 2
B <- spatial_hist / conf_matrix_plot +   plot_layout(heights = c(2,1))
tree_plot + B  +   plot_layout(widths = c(2,1)) +  plot_annotation(tag_levels = 'A')

##Arrangement 3
(spatial_hist / tree_plot + inset_element(conf_matrix_plot, left = 0.67, bottom = 0.67, right = 1, top = 1, align_to = 'full')) +
   plot_layout(heights = c(0.5,1.5))+
   plot_annotation(tag_levels = list(c('A', 'B'), '1'))

```